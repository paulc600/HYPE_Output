{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5df1e40-351f-4e76-8a87-74cc00fb7041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "import xarray as xr\n",
    "from typing import Union, Set, Dict\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e027a2-c625-4588-9aa7-509ed907c59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_HYPE_output(file_path: str\n",
    ") -> pd.DataFrame:\n",
    "    ''' Returns DataFrame after reading the HYPE output .txt file, the first row is dropped because it \n",
    "    contains the labels.\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path: str\n",
    "        Path to the .txt file to be converted\n",
    "                \n",
    "    Outputs\n",
    "    ----------\n",
    "    df: DataFrame\n",
    "        DataFrame containing the info from the .txt file\n",
    "    '''\n",
    "    try:\n",
    "        # Read the .txt file and create a DataFrame\n",
    "        df = pd.read_csv(file_path, delimiter='\\t',skiprows=1)  # Assuming tab-separated values in the file\n",
    "        \n",
    "        \n",
    "        # Assuming 'df' is your DataFrame\n",
    "        df = df.drop(df.columns[-1], axis=1)\n",
    "\n",
    "        # Return the DataFrame\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found!\")\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d644d5-819e-4ba0-a97c-5cb5dac8eed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_Temp_Prec(file_path: str\n",
    ") -> pd.DataFrame:\n",
    "    ''' Returns DataFrame after reading Temperature or Precipitaiton.txt file. The IDs are removed from \n",
    "    the index and converted to int in order to sort them against a set of int later. \n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path: str\n",
    "        Path to the .txt file to be converted\n",
    "                \n",
    "    Outputs\n",
    "    ----------\n",
    "    df: DataFrame\n",
    "        DataFrame containing the info from the .txt file\n",
    "    '''\n",
    "    try:\n",
    "        # Read the .txt file and create a DataFrame\n",
    "        df = pd.read_csv(file_path, delimiter='\\t')  # Assuming tab-separated values in the file\n",
    "\n",
    "        # remove IDs from the index\n",
    "        df = df.reset_index()\n",
    "        df.loc[-1] = df.columns\n",
    "        df.sort_index(inplace=True)\n",
    "        df.drop('index', axis=1, inplace=True)\n",
    "        \n",
    "        # convert IDs to int ignoring the first column\n",
    "        df.iloc[0, 1:] = df.iloc[0, 1:].astype(int)\n",
    "        \n",
    "            # Reset the index of the DataFrame\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "        # Return the DataFrame\n",
    "        return df\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found!\")\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47be8d9e-75aa-426a-ad10-5b3ed11bb6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_excel_to_dataframe(file_path, sheet_name=None, start_date=None, end_date=None, column_index=None):\n",
    "    \"\"\"\n",
    "    Read an Excel spreadsheet, trim it based on start and end dates and a column index,\n",
    "    and save the trimmed data to a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): The path to the Excel file.\n",
    "    - sheet_name (str, optional): The name of the sheet to read. If not provided, the first sheet is read.\n",
    "    - start_date (str, optional): The start date for trimming the data. Format: 'YYYY-MM-DD'.\n",
    "    - end_date (str, optional): The end date for trimming the data. Format: 'YYYY-MM-DD'.\n",
    "    - column_index (int, optional): The index number of the column to include in the trimmed DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - df (pandas.DataFrame): The trimmed DataFrame containing the specified data from the Excel spreadsheet.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the file_path is valid\n",
    "    try:\n",
    "        pd.ExcelFile(file_path)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"File '{file_path}' not found.\")\n",
    "\n",
    "    # Read the Excel file\n",
    "    try:\n",
    "        if sheet_name is None:\n",
    "            df = pd.read_excel(file_path, skiprows=3)\n",
    "        else:\n",
    "            df = pd.read_excel(file_path, sheet_name=sheet_name, skiprows=3)\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error reading Excel file: {str(e)}\")\n",
    "\n",
    "    # Trim the DataFrame based on start and end dates\n",
    "    if start_date and end_date:\n",
    "        start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "        end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "        df.iloc[:, 0] = pd.to_datetime(df.iloc[:, 0])\n",
    "        df = df[(df.iloc[:, 0] >= start_date) & (df.iloc[:, 0] <= end_date)]\n",
    "\n",
    "    # Trim the DataFrame based on column index\n",
    "    if column_index is not None:\n",
    "        df = df.iloc[:, [0, column_index]]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166a5124-1cd4-49a6-abc9-b1bbd4298289",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def shapefile_ID(\n",
    "    modifiedcat= str,\n",
    "    riv= str\n",
    ") -> gpd.GeoDataFrame:\n",
    "    \n",
    "    '''\n",
    "    Reads the modified catchment shapefile where IDs have been assigned to all rivers and the river\n",
    "    shapefile. It then concatinates the extra river IDs with the river shapefile. It then extracts \n",
    "    the seg_nhm and ds_nhm from the river shapefile and returns those two columns in a geodataframe. \n",
    "    That information can then be used to find precipitation at a given ID. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    modifiedcat: str\n",
    "        Path to the modified catchment shapefile where the river ID of 0 have been replaced with IDs\n",
    "    riv= str\n",
    "        Path to the river shapefile\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    gpd: GeoDataFrame\n",
    "        Geodataframe with the river IDs and corresponding downriver ID \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Read the modified catchment shapefile\n",
    "    modifiedcat = gpd.read_file(modifiedcat)\n",
    "    # Read the river shapefile\n",
    "    riv = gpd.read_file(riv)\n",
    "    \n",
    "    # Sort the GeoDataFrame by a specific column\n",
    "    sorted_modifiedcat = modifiedcat.sort_values(by='seg_nhm')\n",
    "    sorted_riv = riv.sort_values(by='seg_nhm')\n",
    "    \n",
    "    # Select the specific rows based on the range in a column\n",
    "    selected_rows = modifiedcat[(modifiedcat['seg_nhm'] >= 58662 )]\n",
    "    \n",
    "    # Concatenate the selected rows with the river shapefile\n",
    "    merged_data = pd.concat([riv, selected_rows], ignore_index=True)\n",
    "\n",
    "    # Convert the merged data to a GeoDataFrame\n",
    "    merged_riv = gpd.GeoDataFrame(merged_data, crs=riv.crs)\n",
    "    \n",
    "    #Fill Na in the data\n",
    "    merged_riv=merged_riv.fillna(0)\n",
    "    \n",
    "    # Assuming you have a GeoDataFrame named 'gdf'\n",
    "    merged_riv['seg_nhm'] = merged_riv['seg_nhm'].astype(int)\n",
    "    merged_riv['ds_seg_nhm'] = merged_riv['ds_seg_nhm'].astype(int)\n",
    "    \n",
    "    return merged_riv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd2a581-5686-4207-bd93-1577af30b576",
   "metadata": {},
   "source": [
    "From Kasra: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fd21ad-e982-4186-a2a1-288a9dc517e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_upstream(\n",
    "    gdf: gpd.GeoDataFrame,\n",
    "    target_id: str,\n",
    "    main_id: str,\n",
    "    ds_main_id: str,\n",
    ") -> Set:\n",
    "    '''Find \"ancestors\" or upstream segments in a river network given\n",
    "    in the from of a geopandas.GeoDataFrame `gdf`\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    gdf: geopandas.GeoDataFrame\n",
    "        GeoDataFrame of river segments including at least three pieces\n",
    "        of information: 1) geometries of segments, 2) segment IDs, and\n",
    "        3) downstream segment IDs\n",
    "    target_id: str, int, or any other data type as included in `gdf`\n",
    "        Indicating the target ID anscestor or upstream of which is\n",
    "        desired\n",
    "    main_id: str\n",
    "        String defining the column of element IDs in the input geopandas\n",
    "        dataframe\n",
    "    ds_main_id: str\n",
    "        String defining the column of downstream element IDs in the\n",
    "        input geopandas dataframe\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    nodes: list\n",
    "        IDs of nodes being upstream or anscestor of the `target_id`\n",
    "    \n",
    "    '''\n",
    "    # creating a DiGraph out of `gdf` object\n",
    "    riv_graph = nx.from_pandas_edgelist(gdf, source=main_id, target=ds_main_id, create_using=nx.DiGraph)\n",
    "    \n",
    "    # return nodes in a list\n",
    "    nodes = nx.ancestors(riv_graph, target_id)\n",
    "\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da576972-389d-49f9-b869-798cbb865614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upstream_precipitation(\n",
    "    df: pd.DataFrame, \n",
    "    columns: set, \n",
    "    target_id: int\n",
    ") -> pd.DataFrame:\n",
    "    '''\n",
    "    Finds total precipitation at a station by summing the daily precipitation from all of it's upstream\n",
    "    rivers. This returns a DataFrame with the dates in one column and the sum of precipitation at the \n",
    "    target ID in another column.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: pd.DataFrame\n",
    "        DataFrame containing the Precipitation data (pobs.txt in hype)\n",
    "    columns: set\n",
    "        Set of column names (station IDs) that are upstream of target_id and must be summed\n",
    "    target_id: int\n",
    "        Target ID for analysis. This will be included in the sum\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with dates in the first column and summed precipitation values for the \n",
    "        target ID in the second column\n",
    "    '''\n",
    "    # Filter columns by including the first one (dates), checking against upstream lists and target_id\n",
    "    filtered_columns = [col for col in df.columns[1:] if int(col) in columns or int(col) == target_id]\n",
    "        \n",
    "    # Sum precipitation values for each date from row 1 onwards\n",
    "    summed_precipitation = df.iloc[1:, 1:][filtered_columns].sum(axis=1)\n",
    "        \n",
    "    # Create a new DataFrame with dates and summed precipitation\n",
    "    result_df = pd.DataFrame({'Dates': df.iloc[1:, 0], str(target_id): summed_precipitation})\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c2fd33-0aa1-46e0-909e-dfed48b9a5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weekly_flowrates(calculated_df: pd.DataFrame, natural_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "       Converts the daily flows from the HYPE output to weekly flows for comparison with natural flows.\n",
    "    This function ensures that the dates in the converted weekly flows file matches the dates in natural flows.\n",
    "    Can also be used to convert precipitation into weekly. \n",
    "\n",
    "    Args:\n",
    "        calculated_df (pd.DataFrame): DataFrame containing daily flowrates with the date in the first column.\n",
    "        natural_df (pd.DataFrame): DataFrame containing weekly flowrates with the date in the first column.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with the same weekly dates as natural_df and the updated weekly flowrates.\n",
    "\n",
    "    \"\"\"\n",
    "    # Set the date column as the index for calculated_df\n",
    "    calculated_df.index = pd.to_datetime(calculated_df.iloc[:, 0])\n",
    "    calculated_df.drop(calculated_df.columns[0], axis=1, inplace=True)\n",
    "    #calculated_df.drop(calculated_df.columns[1], axis=1, inplace=True)  # Drop the third column\n",
    "\n",
    "    # Set the date column as the index for natural_df\n",
    "    natural_df.index = pd.to_datetime(natural_df.iloc[:, 0])\n",
    "    natural_df.drop(natural_df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "    # Create a new DataFrame with the index of natural_df and the weekly flowrates\n",
    "    results_df = pd.DataFrame(index=natural_df.index, columns=['WeeklyFlow'])\n",
    "\n",
    "    # Iterate through the rows of natural_df starting from the second row\n",
    "    for i in range(1, len(natural_df)):\n",
    "        # Find the corresponding date in calculated_df\n",
    "        date = natural_df.index[i]\n",
    "\n",
    "        # Find the previous date in natural_df\n",
    "        prev_date = natural_df.index[i - 1]\n",
    "\n",
    "        # Find the corresponding rows in calculated_df\n",
    "        calculated_rows = calculated_df[(calculated_df.index > prev_date) & (calculated_df.index <= date)]\n",
    "\n",
    "        # Sum the flowrates in the calculated rows\n",
    "        weekly_flow = calculated_rows.sum().values[0]\n",
    "\n",
    "        # Save the weekly flowrate in results_df\n",
    "        results_df.loc[date] = weekly_flow\n",
    "\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e452aff-6b06-486f-ade1-d73609fd9ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hydrograph_with_precipitation(df1: pd.DataFrame, df2: pd.DataFrame, df3: pd.DataFrame, station_id: int):\n",
    "    \"\"\"\n",
    "    Plots hydrographs from two dataframes on top of each other and precipitation as an upside-down bar plot above them.\n",
    "\n",
    "    Args:\n",
    "        df1 (pd.DataFrame): Dataframe containing hydrograph data.\n",
    "        df2 (pd.DataFrame): Dataframe containing hydrograph data.\n",
    "        df3 (pd.DataFrame): Dataframe containing precipitation data.\n",
    "        station_id (int): Station ID to be added to the plot title.\n",
    "    \"\"\"\n",
    "    \n",
    "        # Convert index to datetime format\n",
    "    df1.index = pd.to_datetime(df1.index)\n",
    "    df2.index = pd.to_datetime(df2.index)\n",
    "    df3.index = pd.to_datetime(df3.index)\n",
    "    \n",
    "    # Create a new figure and axis with a larger size\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "            # Rotate x-axis labels by 45 degrees\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Plot the first hydrograph\n",
    "    ax.plot(df1.index, df1.iloc[:, 0], color='blue', label='Hydrograph 1')\n",
    "    ax.set_ylabel('Hydrograph (m$^3$/s)', color='blue')\n",
    "    ax.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "    # Create a second y-axis for precipitation\n",
    "    ax2 = ax.twinx()\n",
    "\n",
    "    # Plot the second hydrograph on the left y-axis\n",
    "    ax.plot(df2.index, df2.iloc[:, 0], color='red', label='Hydrograph 2')\n",
    "    ax.set_ylabel('Hydrograph (m$^3$/s)', color='red')\n",
    "    ax.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "    # Plot precipitation as upside-down bar plot on the right y-axis\n",
    "    ax2.bar(df3.index, -df3.iloc[:, 0], width=0.8, color='gray', alpha=0.5, label='Precipitation')\n",
    "    ax2.set_ylabel('Precipitation (mm/week)', color='gray')\n",
    "    ax2.tick_params(axis='y', labelcolor='gray')\n",
    "\n",
    "    # Invert the y-axis for precipitation\n",
    "    ax2.invert_yaxis()\n",
    "\n",
    "    # Set the labels and title\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_title(f'Hydrograph with Precipitation (Station ID: {station_id})')\n",
    "\n",
    "    # Adjust x-axis tick labels by year\n",
    "    #years = pd.DatetimeIndex(df1.index).year\n",
    "    #ax.set_xticks(df1.index)\n",
    "    #ax.set_xticklabels(years, rotation=45, ha='right')\n",
    "\n",
    "    ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))\n",
    "    \n",
    "    # Flip the right y-axis scale\n",
    "    ax2.invert_yaxis()\n",
    "\n",
    "    # Combine the legends\n",
    "    lines1, labels1 = ax.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "    # Adjust layout to prevent overlapping\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5be8c44-58d5-442d-96e9-3e70854f0e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_id= 58308"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfb5899-363b-478f-8d42-d6fe5a5ee8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nat= read_excel_to_dataframe('/home/paulc600/local/Nat_flow_update_Prabin_2023_03_13.xlsx', 'Nat flow_1909-2021',\n",
    "                            '1980-01-01', '2018-12-31', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c810cce-bc30-42b5-9081-2af237aa255c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30156f7-593f-416d-a819-25c7b1e1081c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = import_HYPE_output('/home/paulc600/local/02_HYPE_outputs/0058308.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a0a432-ad50-4b3f-90d5-c4ee95f461a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc49618-300a-4b9a-84de-a4d746930b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "pobs=import_Temp_Prec('/home/paulc600/local/02_HYPE_inputs/Pobs.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64537e0-ce5c-470f-b2d3-6c55f87638ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ID=shapefile_ID('/home/paulc600/SMM/SMM HYPE files/Modified_SMMcat.shp',\n",
    "                  '/home/paulc600/github/StMaryMilk2023-UofC/modified_TGF/smm_tgf_modified/smm_riv.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3069c98c-7dcb-4dc9-82a2-8ba0737459dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "upstream = find_upstream(ID, target_id, 'seg_nhm', 'ds_seg_nhm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8113d278-2aa0-4955-9838-c1a9af9f65d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "summed_precipitation=upstream_precipitation(pobs, upstream, target_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7af7fb-cbf2-4dcf-92e8-04bb10a421ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_calc= update_weekly_flowrates(data_frame, nat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231e2809-2f43-4dfa-a948-b1a95ff4ff56",
   "metadata": {},
   "outputs": [],
   "source": [
    "nat.insert(0, 'Dates', nat.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cbb8e2-3c79-4f78-928a-e7f3a1d8e211",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_prec= update_weekly_flowrates(summed_precipitation, nat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d27495-b79c-4222-ab24-8ae6c53cc54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hydrograph_with_precipitation(weekly_calc, nat, summed_precipitation, target_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86af40de-740b-4212-84a1-abe96d8aab78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "easymore-env",
   "language": "python",
   "name": "easymore-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
