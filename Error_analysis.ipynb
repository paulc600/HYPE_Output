{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bcc9ba-c89e-44da-a57e-788f57f7aac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydrostats as hs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy.ma as ma\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5192d0be-2dd9-4a13-b316-c752482259be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_HYPE_output(file_path: str\n",
    ") -> pd.DataFrame:\n",
    "    ''' Returns DataFrame after reading the HYPE output .txt file, the first row is dropped because it \n",
    "    contains the labels.\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path: str\n",
    "        Path to the .txt file to be converted\n",
    "                \n",
    "    Outputs\n",
    "    ----------\n",
    "    df: DataFrame\n",
    "        DataFrame containing the info from the .txt file\n",
    "    '''\n",
    "    try:\n",
    "        # Read the .txt file and create a DataFrame\n",
    "        df = pd.read_csv(file_path, delimiter='\\t',skiprows=1)  # Assuming tab-separated values in the file\n",
    "\n",
    "        # Return the DataFrame\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found!\")\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e473b7d2-105c-43da-bba6-167e1e4785ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_excel_to_dataframe(\n",
    "    file_path: str,                             \n",
    "    sheet_name: str, \n",
    "    start_date: str, \n",
    "    end_date: str, \n",
    "    column_index: int\n",
    ")-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read an Excel spreadsheet, trims it based on start and end dates and a column index,\n",
    "    and save the trimmed data to a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): The path to the Excel file.\n",
    "    - sheet_name (str, optional): The name of the sheet to read. If not provided, the first sheet is read.\n",
    "    - start_date (str, optional): The start date for trimming the data. Format: 'YYYY-MM-DD'.\n",
    "    - end_date (str, optional): The end date for trimming the data. Format: 'YYYY-MM-DD'.\n",
    "    - column_index (int, optional): The index number of the column to include in the trimmed DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - df (pandas.DataFrame): The trimmed DataFrame containing the specified data from the Excel spreadsheet.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the file_path is valid\n",
    "    try:\n",
    "        pd.ExcelFile(file_path)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"File '{file_path}' not found.\")\n",
    "\n",
    "    # Read the Excel file\n",
    "    try:\n",
    "        if sheet_name is None:\n",
    "            df = pd.read_excel(file_path, skiprows=3)\n",
    "        else:\n",
    "            df = pd.read_excel(file_path, sheet_name=sheet_name, skiprows=3)\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error reading Excel file: {str(e)}\")\n",
    "\n",
    "    # Trim the DataFrame based on start and end dates\n",
    "    if start_date and end_date:\n",
    "        start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "        end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "        df.iloc[:, 0] = pd.to_datetime(df.iloc[:, 0])\n",
    "        df = df[(df.iloc[:, 0] >= start_date) & (df.iloc[:, 0] <= end_date)]\n",
    "\n",
    "    # Trim the DataFrame based on column index\n",
    "    if column_index is not None:\n",
    "        df = df.iloc[:, [0, column_index]]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2eb5820-9e70-437f-8a00-b665e1c15cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weekly_flowrates(\n",
    "    calculated_df: pd.DataFrame, \n",
    "    natural_df: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "       Converts the daily flows from the HYPE output to weekly flows for comparison with natural flows.\n",
    "    This function ensures that the dates in the converted weekly flows file matches the dates in natural flows.\n",
    "    Weekly flow is found by calculating the mean flowrate over from the given date to the next given date. \n",
    "\n",
    "    Args:\n",
    "        calculated_df: pd.DataFrame \n",
    "            DataFrame containing calculated daily flowrates with the date in the first column.\n",
    "        natural_df: pd.DataFrame \n",
    "            DataFrame containing natural weekly flowrates with the date in the first column.\n",
    "\n",
    "    Returns:\n",
    "        results_df: pd.DataFrame \n",
    "            DataFrame with the same weekly dates as natural_df and the updated weekly flowrates.\n",
    "\n",
    "    \"\"\"\n",
    "    # Set the date column as the index for calculated_df\n",
    "    calculated_df.index = pd.to_datetime(calculated_df.iloc[:, 0])\n",
    "    calculated_df.drop(calculated_df.columns[0], axis=1, inplace=True)\n",
    "    #calculated_df.drop(calculated_df.columns[1], axis=1, inplace=True)  # Drop the third column\n",
    "\n",
    "    # Set the date column as the index for natural_df\n",
    "    natural_df.index = pd.to_datetime(natural_df.iloc[:, 0])\n",
    "    natural_df.drop(natural_df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "    # Create a new DataFrame with the index of natural_df and the weekly flowrates\n",
    "    results_df = pd.DataFrame(index=natural_df.index, columns=['WeeklyFlow'])\n",
    "\n",
    "    # Iterate through the rows of natural_df starting from the first row\n",
    "    for i in range(0, len(natural_df) -1):\n",
    "        \n",
    "        # Find the corresponding date in natural_df\n",
    "        date = natural_df.index[i]\n",
    "\n",
    "        # Find the next date in natural_df\n",
    "        next_date = natural_df.index[i + 1]\n",
    "\n",
    "        # Find the corresponding rows in calculated_df\n",
    "        calculated_rows = calculated_df[(calculated_df.index >= date) & (calculated_df.index < next_date)]\n",
    "\n",
    "        # Find the mean flowrate in the calculated rows\n",
    "        weekly_flow = calculated_rows.mean().values[0]\n",
    "\n",
    "        # Save the weekly flowrate in results_df\n",
    "        results_df.loc[date] = weekly_flow\n",
    "\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1144169-bd5b-416f-9557-2be19e8492fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nan_rows(\n",
    "    array1: np.ndarray, \n",
    "    array2: np.ndarray\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Removes rows from two arrays where either array has NaN values.\n",
    "    Retains the first row if it doesn't have any NaN values.\n",
    "    \n",
    "    Arguments:\n",
    "    array1: np.ndarray:\n",
    "        First input array\n",
    "    array2: np.ndarray\n",
    "        Second input array\n",
    "    \n",
    "    Returns:\n",
    "    cleaned_array1: : np.ndarray\n",
    "        Cleaned array1 without NaN rows\n",
    "    cleaned_array2: np.ndarray\n",
    "        Cleaned array2 without NaN rows\n",
    "    \"\"\"\n",
    "    # checks for and removes any rows where either array has a value of NaN at a corresponding row \n",
    "    # including the first one\n",
    "    \n",
    "    mask = np.logical_and(~np.isnan(array1), ~np.isnan(array2))\n",
    "    if not np.isnan(array1[0]) and not np.isnan(array2[0]):\n",
    "        mask[0] = True\n",
    "    cleaned_array1 = array1[mask]\n",
    "    cleaned_array2 = array2[mask]\n",
    "    return cleaned_array1, cleaned_array2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25caa8e-9114-4ff2-9f9f-0ae08331eaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPE_output = import_HYPE_output('/home/paulc600/local/HYPE/out/0058308.txt')\n",
    "HYPE_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06230487-a94f-414f-9cfd-03a9a91d4403",
   "metadata": {},
   "outputs": [],
   "source": [
    "nat = read_excel_to_dataframe('/home/paulc600/local/Nat_flow_update_Prabin_2023_03_13.xlsx', 'Nat flow_1909-2021',\n",
    "                             '1980-01-01', '2018-12-31', 4)\n",
    "nat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608693ee-4f3b-4eb3-8b7a-e72e9175d15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly=update_weekly_flowrates(HYPE_output, nat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206f7901-3a95-4b77-b89b-8e2b097739d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6d2145-c755-4b6c-912b-9121b0fdafff",
   "metadata": {},
   "outputs": [],
   "source": [
    "observed= weekly.iloc[ : , 0]\n",
    "observed= observed.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636bb3a7-ccb7-419e-b96f-bfb7872f235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72b8271-2be4-4a50-9ed7-a76c7b5840a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasted= nat.iloc[ : , 0]\n",
    "forecasted= forecasted.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94123cd-fe95-446e-a02f-dd8f14bb076e",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe09a50-6a6a-4d8c-860f-dcbed498a078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert observed and forecasted data to NumPy arrays with float data type\n",
    "observed_array = np.array(observed, dtype=float)\n",
    "forecasted_array = np.array(forecasted, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83198d00-2c8b-412d-9052-992f73f64a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_clean, forecasted_clean= remove_nan_rows(observed_array, forecasted_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2784e1fa-3899-424f-9505-b94ffc45c8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean squared error\n",
    "mse_value = hs.mse(observed_clean, forecasted_clean)\n",
    "print(\"Mean Squared Error:\", mse_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3369d67-0db1-4228-91f6-a87af2bab2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nse_value = hs.nse(observed_clean, forecasted_clean)\n",
    "print(\"Nash-Sutcliffe Efficiency:\", nse_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f69539-5069-4b87-850b-a4707e9fd6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "kge_value = hs.kge_2012(observed_clean, forecasted_clean)\n",
    "print(\"Kling-Gupta Efficiency (2012):\", kge_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0254a97-8361-4743-9ad8-9eab338fd2f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "easymore-env",
   "language": "python",
   "name": "easymore-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
